{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd03e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/mac/Documents/UR_RAG_Code/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk --upgrade --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK version: 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"‚úÖ NLTK version:\", nltk.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77827e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/mac/Documents/UR_RAG_Code/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas sacrebleu evaluate bert-score urduhack torch transformers stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b63d83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 899 Urdu samples for evaluation.\n",
      "\n",
      "üîç Evaluating: LangChain MultiVector Retriever\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:09<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 148.16it/s]\n",
      "Warning: Baseline not Found for xlm-roberta-base on ur at /Users/mac/Documents/UR_RAG_Code/venv/lib/python3.9/site-packages/bert_score/rescale_baseline/ur/xlm-roberta-base.tsv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.67 seconds, 92.98 sentences/sec\n",
      "‚úÖ Scores calculated for: LangChain MultiVector Retriever\n",
      "\n",
      "üìà === Average Scores for LangChain MultiVector Retriever ===\n",
      "BLEU: 4.7063\n",
      "SacreBLEU: 5.8257\n",
      "ROUGE-1: 0.1569\n",
      "ROUGE-2: 0.0890\n",
      "ROUGE-L: 0.1549\n",
      "METEOR: 0.2555\n",
      "Exact Match: 0.0011\n",
      "BERTScore F1: 0.8057\n",
      "\n",
      "‚úÖ Results saved to: LangChain_MultiVector_NLP_Evaluation_XLMR.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# LangChain MultiVector Retriever - NLP Metrics Evaluation (Urdu, XLM-Roberta)\n",
    "# =============================================\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sacrebleu\n",
    "from evaluate import load\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "import stanza\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Urdu NLP Setup\n",
    "# ---------------------------------------------\n",
    "try:\n",
    "    nlp = stanza.Pipeline('ur', processors='tokenize', use_gpu=torch.cuda.is_available(), verbose=False)\n",
    "except:\n",
    "    stanza.download('ur')\n",
    "    nlp = stanza.Pipeline('ur', processors='tokenize', use_gpu=torch.cuda.is_available(), verbose=False)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Urdu Text Normalization\n",
    "# ---------------------------------------------\n",
    "def normalize_urdu_text(text):\n",
    "    text = text.strip()\n",
    "    # Remove diacritics and unwanted punctuation\n",
    "    text = re.sub(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u06D6-\\u06ED]\", \"\", text)\n",
    "    text = re.sub(r\"[\\u06D4\\u060C\\u066B\\u066C]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Basic synonym normalization for tense consistency\n",
    "    synonym_map = {\"⁄©€åÿß\": \"⁄©€åÿß ÿ™⁄æÿß\", \"€Å€í\": \"ÿ™⁄æÿß\", \"€Å€å⁄∫\": \"ÿ™⁄æ€í\"}\n",
    "    for k, v in synonym_map.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "# Urdu Tokenizer\n",
    "def urdu_tokenizer(text):\n",
    "    text = normalize_urdu_text(text)\n",
    "    doc = nlp(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load Evaluation Metrics\n",
    "# ---------------------------------------------\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load LangChain MultiVector Results CSV\n",
    "# ---------------------------------------------\n",
    "csv_file = 'langchain_multivector_complete_results.csv'  # your generated file\n",
    "df = pd.read_csv(csv_file, usecols=['langchain_refined_answer', 'answer'])\n",
    "df = df.fillna(\"\").astype(str)\n",
    "\n",
    "references = [normalize_urdu_text(ref) for ref in df['answer'].tolist()]\n",
    "multi_preds = [normalize_urdu_text(pred) for pred in df['langchain_refined_answer'].tolist()]\n",
    "\n",
    "print(f\"üìÇ Loaded {len(references)} Urdu samples for evaluation.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Metric Calculation Functions\n",
    "# ---------------------------------------------\n",
    "def calculate_nltk_bleu(references, hypotheses):\n",
    "    scores = []\n",
    "    smooth = SmoothingFunction().method4\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        score = sentence_bleu([urdu_tokenizer(ref)], urdu_tokenizer(hyp), smoothing_function=smooth)\n",
    "        scores.append(score * 100)\n",
    "    return scores\n",
    "\n",
    "def calculate_sacrebleu(references, hypotheses):\n",
    "    return [sacrebleu.sentence_bleu(hyp, [ref]).score for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    results = rouge.compute(\n",
    "        predictions=hypotheses,\n",
    "        references=references,\n",
    "        tokenizer=lambda x: urdu_tokenizer(x),\n",
    "        use_aggregator=False\n",
    "    )\n",
    "    return results['rouge1'], results['rouge2'], results['rougeL']\n",
    "\n",
    "def calculate_meteor(references, hypotheses):\n",
    "    ref_tok = [urdu_tokenizer(ref) for ref in references]\n",
    "    hyp_tok = [urdu_tokenizer(hyp) for hyp in hypotheses]\n",
    "    return [meteor_score([r], h) for r, h in zip(ref_tok, hyp_tok)]\n",
    "\n",
    "def calculate_bert_score(references, hypotheses):\n",
    "    # ‚úÖ Using XLM-Roberta-Base for token-level similarity\n",
    "    P, R, F1 = bert_score(\n",
    "        cands=hypotheses,\n",
    "        refs=references,\n",
    "        lang=\"ur\",\n",
    "        model_type=\"xlm-roberta-base\",\n",
    "        rescale_with_baseline=True,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        verbose=True,\n",
    "    )\n",
    "    return P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "def calculate_exact_match(references, hypotheses):\n",
    "    return [1 if ref.strip() == hyp.strip() else 0 for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Evaluation Function\n",
    "# ---------------------------------------------\n",
    "def evaluate_pipeline(name, references, predictions):\n",
    "    print(f\"\\nüîç Evaluating: {name}\")\n",
    "\n",
    "    nltk_bleu = calculate_nltk_bleu(references, predictions)\n",
    "    sacrebleu_scores = calculate_sacrebleu(references, predictions)\n",
    "    rouge1, rouge2, rougeL = calculate_rouge(references, predictions)\n",
    "    meteor = calculate_meteor(references, predictions)\n",
    "    exact_match = calculate_exact_match(references, predictions)\n",
    "    bert_p, bert_r, bert_f1 = calculate_bert_score(references, predictions)\n",
    "\n",
    "    print(f\"‚úÖ Scores calculated for: {name}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Pipeline': [name] * len(references),\n",
    "        'Ground Truth Answer': references,\n",
    "        'Generated Answer': predictions,\n",
    "        'BLEU': nltk_bleu,\n",
    "        'SacreBLEU': sacrebleu_scores,\n",
    "        'ROUGE-1': rouge1,\n",
    "        'ROUGE-2': rouge2,\n",
    "        'ROUGE-L': rougeL,\n",
    "        'METEOR': meteor,\n",
    "        'Exact Match': exact_match,\n",
    "        'BERT Precision': bert_p,\n",
    "        'BERT Recall': bert_r,\n",
    "        'BERT F1': bert_f1\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Run Evaluation for LangChain MultiVector Retriever\n",
    "# ---------------------------------------------\n",
    "multi_results = evaluate_pipeline(\"LangChain MultiVector Retriever\", references, multi_preds)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Summary Function\n",
    "# ---------------------------------------------\n",
    "def summarize_metrics(df, label):\n",
    "    print(f\"\\nüìà === Average Scores for {label} ===\")\n",
    "    print(f\"BLEU: {df['BLEU'].mean():.4f}\")\n",
    "    print(f\"SacreBLEU: {df['SacreBLEU'].mean():.4f}\")\n",
    "    print(f\"ROUGE-1: {df['ROUGE-1'].mean():.4f}\")\n",
    "    print(f\"ROUGE-2: {df['ROUGE-2'].mean():.4f}\")\n",
    "    print(f\"ROUGE-L: {df['ROUGE-L'].mean():.4f}\")\n",
    "    print(f\"METEOR: {df['METEOR'].mean():.4f}\")\n",
    "    print(f\"Exact Match: {df['Exact Match'].mean():.4f}\")\n",
    "    print(f\"BERTScore F1: {df['BERT F1'].mean():.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Display Results & Save\n",
    "# ---------------------------------------------\n",
    "summarize_metrics(multi_results, \"LangChain MultiVector Retriever\")\n",
    "\n",
    "output_csv = \"LangChain_MultiVector_NLP_Evaluation_XLMR.csv\"\n",
    "multi_results.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n‚úÖ Results saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e27097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 1205 English samples for evaluation.\n",
      "\n",
      "üîç Evaluating: LangChain MultiVector Retriever (English)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:53<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 95.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 53.46 seconds, 22.54 sentences/sec\n",
      "‚úÖ Scores calculated for: LangChain MultiVector Retriever (English)\n",
      "\n",
      "üìà === Average Scores for LangChain MultiVector Retriever (English) ===\n",
      "BLEU: 11.3382\n",
      "SacreBLEU: 15.0434\n",
      "ROUGE-1: 0.3072\n",
      "ROUGE-2: 0.1960\n",
      "ROUGE-L: 0.3060\n",
      "METEOR: 0.4747\n",
      "Exact Match: 0.0498\n",
      "BERTScore F1: 0.2160\n",
      "\n",
      "‚úÖ Results saved to: LangChain_MultiVector_NLP_Evaluation_English.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# LangChain MultiVector Retriever - NLP Metrics Evaluation (English, RoBERTa)\n",
    "# =============================================\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sacrebleu\n",
    "from evaluate import load\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------\n",
    "# English NLP Setup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "\n",
    "def normalize_english_text(text):\n",
    "    \"\"\"Basic normalization: lowercase, strip punctuation, and extra spaces\"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def english_tokenizer(text):\n",
    "    \"\"\"Tokenize English text using NLTK\"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load Evaluation Metrics\n",
    "# ---------------------------------------------\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load LangChain MultiVector Results CSV\n",
    "# ---------------------------------------------\n",
    "csv_file = 'langchain_multivector_english_results.csv'  # your generated file\n",
    "df = pd.read_csv(csv_file, usecols=['langchain_refined_answer', 'answers'])\n",
    "df = df.fillna(\"\").astype(str)\n",
    "\n",
    "references = [normalize_english_text(ref) for ref in df['answers'].tolist()]\n",
    "multi_preds = [normalize_english_text(pred) for pred in df['langchain_refined_answer'].tolist()]\n",
    "\n",
    "print(f\"üìÇ Loaded {len(references)} English samples for evaluation.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Metric Calculation Functions\n",
    "# ---------------------------------------------\n",
    "def calculate_nltk_bleu(references, hypotheses):\n",
    "    scores = []\n",
    "    smooth = SmoothingFunction().method4\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_tokens = english_tokenizer(ref)\n",
    "        hyp_tokens = english_tokenizer(hyp)\n",
    "        score = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth)\n",
    "        scores.append(score * 100)\n",
    "    return scores\n",
    "\n",
    "def calculate_sacrebleu(references, hypotheses):\n",
    "    return [sacrebleu.sentence_bleu(hyp, [ref]).score for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    results = rouge.compute(\n",
    "        predictions=hypotheses,\n",
    "        references=references,\n",
    "        tokenizer=lambda x: english_tokenizer(x),\n",
    "        use_aggregator=False\n",
    "    )\n",
    "    return results['rouge1'], results['rouge2'], results['rougeL']\n",
    "\n",
    "def calculate_meteor(references, hypotheses):\n",
    "    ref_tok = [english_tokenizer(ref) for ref in references]\n",
    "    hyp_tok = [english_tokenizer(hyp) for hyp in hypotheses]\n",
    "    return [meteor_score([r], h) for r, h in zip(ref_tok, hyp_tok)]\n",
    "\n",
    "def calculate_bert_score(references, hypotheses):\n",
    "    # ‚úÖ Using RoBERTa-Large for English semantic similarity\n",
    "    P, R, F1 = bert_score(\n",
    "        cands=hypotheses,\n",
    "        refs=references,\n",
    "        lang=\"en\",\n",
    "        model_type=\"roberta-large\",\n",
    "        rescale_with_baseline=True,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        verbose=True,\n",
    "    )\n",
    "    return P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "def calculate_exact_match(references, hypotheses):\n",
    "    return [1 if ref.strip() == hyp.strip() else 0 for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Evaluation Function\n",
    "# ---------------------------------------------\n",
    "def evaluate_pipeline(name, references, predictions):\n",
    "    print(f\"\\nüîç Evaluating: {name}\")\n",
    "\n",
    "    nltk_bleu = calculate_nltk_bleu(references, predictions)\n",
    "    sacrebleu_scores = calculate_sacrebleu(references, predictions)\n",
    "    rouge1, rouge2, rougeL = calculate_rouge(references, predictions)\n",
    "    meteor = calculate_meteor(references, predictions)\n",
    "    exact_match = calculate_exact_match(references, predictions)\n",
    "    bert_p, bert_r, bert_f1 = calculate_bert_score(references, predictions)\n",
    "\n",
    "    print(f\"‚úÖ Scores calculated for: {name}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Pipeline': [name] * len(references),\n",
    "        'Ground Truth Answer': references,\n",
    "        'Generated Answer': predictions,\n",
    "        'BLEU': nltk_bleu,\n",
    "        'SacreBLEU': sacrebleu_scores,\n",
    "        'ROUGE-1': rouge1,\n",
    "        'ROUGE-2': rouge2,\n",
    "        'ROUGE-L': rougeL,\n",
    "        'METEOR': meteor,\n",
    "        'Exact Match': exact_match,\n",
    "        'BERT Precision': bert_p,\n",
    "        'BERT Recall': bert_r,\n",
    "        'BERT F1': bert_f1\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Run Evaluation for LangChain MultiVector Retriever\n",
    "# ---------------------------------------------\n",
    "multi_results = evaluate_pipeline(\"LangChain MultiVector Retriever (English)\", references, multi_preds)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Summary Function\n",
    "# ---------------------------------------------\n",
    "def summarize_metrics(df, label):\n",
    "    print(f\"\\nüìà === Average Scores for {label} ===\")\n",
    "    print(f\"BLEU: {df['BLEU'].mean():.4f}\")\n",
    "    print(f\"SacreBLEU: {df['SacreBLEU'].mean():.4f}\")\n",
    "    print(f\"ROUGE-1: {df['ROUGE-1'].mean():.4f}\")\n",
    "    print(f\"ROUGE-2: {df['ROUGE-2'].mean():.4f}\")\n",
    "    print(f\"ROUGE-L: {df['ROUGE-L'].mean():.4f}\")\n",
    "    print(f\"METEOR: {df['METEOR'].mean():.4f}\")\n",
    "    print(f\"Exact Match: {df['Exact Match'].mean():.4f}\")\n",
    "    print(f\"BERTScore F1: {df['BERT F1'].mean():.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Display Results & Save\n",
    "# ---------------------------------------------\n",
    "summarize_metrics(multi_results, \"LangChain MultiVector Retriever (English)\")\n",
    "\n",
    "output_csv = \"LangChain_MultiVector_NLP_Evaluation_English.csv\"\n",
    "multi_results.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n‚úÖ Results saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef026bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded 1205 English samples for evaluation.\n",
      "\n",
      "üîç Evaluating: LangChain MultiVector Retriever (English)\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [03:05<00:00,  5.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 186.51 seconds, 6.46 sentences/sec\n",
      "‚úÖ Scores calculated for: LangChain MultiVector Retriever (English)\n",
      "\n",
      "üìà === Average Scores for LangChain MultiVector Retriever (English) ===\n",
      "BLEU: 0.1134\n",
      "SacreBLEU: 0.1504\n",
      "ROUGE-1: 0.3262\n",
      "ROUGE-2: 0.2115\n",
      "ROUGE-L: 0.3249\n",
      "METEOR: 0.4747\n",
      "Exact Match: 0.0498\n",
      "BERTScore F1: 0.1903\n",
      "\n",
      "‚úÖ Results saved to: LangChain_MultiVector_NLP_Evaluation_English1.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# LangChain MultiVector Retriever - NLP Metrics Evaluation (English, DeBERTa)\n",
    "# =============================================\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sacrebleu\n",
    "from evaluate import load\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# ---------------------------------------------\n",
    "# English NLP Setup\n",
    "# ---------------------------------------------\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# English Text Normalization\n",
    "# ---------------------------------------------\n",
    "def normalize_english_text(text):\n",
    "    \"\"\"Normalize English text by lowercasing, removing punctuation, and trimming spaces.\"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s.,!?']\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def english_tokenizer(text):\n",
    "    \"\"\"Tokenize English text using NLTK.\"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load Evaluation Metrics\n",
    "# ---------------------------------------------\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Load LangChain MultiVector Results CSV\n",
    "# ---------------------------------------------\n",
    "csv_file = 'langchain_multivector_english_results.csv'  # your results file\n",
    "df = pd.read_csv(csv_file, usecols=['langchain_refined_answer', 'answers'])\n",
    "df = df.fillna(\"\").astype(str)\n",
    "\n",
    "references = [normalize_english_text(ref) for ref in df['answers'].tolist()]\n",
    "multi_preds = [normalize_english_text(pred) for pred in df['langchain_refined_answer'].tolist()]\n",
    "\n",
    "print(f\"üìÇ Loaded {len(references)} English samples for evaluation.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Metric Calculation Functions\n",
    "# ---------------------------------------------\n",
    "def calculate_nltk_bleu(references, hypotheses):\n",
    "    scores = []\n",
    "    smooth = SmoothingFunction().method4\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_tokens = english_tokenizer(ref)\n",
    "        hyp_tokens = english_tokenizer(hyp)\n",
    "        if len(hyp_tokens) == 0 or len(ref_tokens) == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        score = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smooth)\n",
    "        scores.append(score)  # scaled 0‚Äì1\n",
    "    return scores\n",
    "\n",
    "def calculate_sacrebleu(references, hypotheses):\n",
    "    \"\"\"Return SacreBLEU normalized between 0‚Äì1\"\"\"\n",
    "    return [sacrebleu.sentence_bleu(hyp, [ref]).score / 100 for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    \"\"\"ROUGE scores computed using HF evaluate.\"\"\"\n",
    "    results = rouge.compute(\n",
    "        predictions=hypotheses,\n",
    "        references=references,\n",
    "        use_aggregator=False\n",
    "    )\n",
    "    return results['rouge1'], results['rouge2'], results['rougeL']\n",
    "\n",
    "def calculate_meteor(references, hypotheses):\n",
    "    ref_tok = [english_tokenizer(ref) for ref in references]\n",
    "    hyp_tok = [english_tokenizer(hyp) for hyp in hypotheses]\n",
    "    scores = []\n",
    "    for r, h in zip(ref_tok, hyp_tok):\n",
    "        try:\n",
    "            scores.append(meteor_score([r], h))\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "def calculate_bert_score(references, hypotheses):\n",
    "    \"\"\"Semantic similarity using DeBERTa-Xlarge-MNLI for English.\"\"\"\n",
    "    P, R, F1 = bert_score(\n",
    "        cands=hypotheses,\n",
    "        refs=references,\n",
    "        lang=\"en\",\n",
    "        model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "        rescale_with_baseline=True,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        verbose=True,\n",
    "    )\n",
    "    return P.tolist(), R.tolist(), F1.tolist()\n",
    "\n",
    "def calculate_exact_match(references, hypotheses):\n",
    "    return [1 if ref.strip() == hyp.strip() else 0 for ref, hyp in zip(references, hypotheses)]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Evaluation Function\n",
    "# ---------------------------------------------\n",
    "def evaluate_pipeline(name, references, predictions):\n",
    "    print(f\"\\nüîç Evaluating: {name}\")\n",
    "\n",
    "    nltk_bleu = calculate_nltk_bleu(references, predictions)\n",
    "    sacrebleu_scores = calculate_sacrebleu(references, predictions)\n",
    "    rouge1, rouge2, rougeL = calculate_rouge(references, predictions)\n",
    "    meteor = calculate_meteor(references, predictions)\n",
    "    exact_match = calculate_exact_match(references, predictions)\n",
    "    bert_p, bert_r, bert_f1 = calculate_bert_score(references, predictions)\n",
    "\n",
    "    print(f\"‚úÖ Scores calculated for: {name}\")\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Pipeline': [name] * len(references),\n",
    "        'Ground Truth Answer': references,\n",
    "        'Generated Answer': predictions,\n",
    "        'BLEU': nltk_bleu,\n",
    "        'SacreBLEU': sacrebleu_scores,\n",
    "        'ROUGE-1': rouge1,\n",
    "        'ROUGE-2': rouge2,\n",
    "        'ROUGE-L': rougeL,\n",
    "        'METEOR': meteor,\n",
    "        'Exact Match': exact_match,\n",
    "        'BERT Precision': bert_p,\n",
    "        'BERT Recall': bert_r,\n",
    "        'BERT F1': bert_f1\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Run Evaluation for LangChain MultiVector Retriever\n",
    "# ---------------------------------------------\n",
    "multi_results = evaluate_pipeline(\"LangChain MultiVector Retriever (English)\", references, multi_preds)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Summary Function\n",
    "# ---------------------------------------------\n",
    "def summarize_metrics(df, label):\n",
    "    print(f\"\\nüìà === Average Scores for {label} ===\")\n",
    "    print(f\"BLEU: {df['BLEU'].mean():.4f}\")\n",
    "    print(f\"SacreBLEU: {df['SacreBLEU'].mean():.4f}\")\n",
    "    print(f\"ROUGE-1: {df['ROUGE-1'].mean():.4f}\")\n",
    "    print(f\"ROUGE-2: {df['ROUGE-2'].mean():.4f}\")\n",
    "    print(f\"ROUGE-L: {df['ROUGE-L'].mean():.4f}\")\n",
    "    print(f\"METEOR: {df['METEOR'].mean():.4f}\")\n",
    "    print(f\"Exact Match: {df['Exact Match'].mean():.4f}\")\n",
    "    print(f\"BERTScore F1: {df['BERT F1'].mean():.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Display Results & Save\n",
    "# ---------------------------------------------\n",
    "summarize_metrics(multi_results, \"LangChain MultiVector Retriever (English)\")\n",
    "\n",
    "output_csv = \"LangChain_MultiVector_NLP_Evaluation_English1.csv\"\n",
    "multi_results.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n‚úÖ Results saved to: {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
