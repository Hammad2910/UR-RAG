{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1731934540989,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "PHoU2URCon7J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23030014\\Thesis Research\\Project Code\\myenv11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\23030014\\Thesis Research\\Project Code\\myenv11\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import PyPDF2\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM-8sNJbnO6r"
   },
   "source": [
    "### **Processing Pdfs Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731934541356,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "3Y5WGWOVnHy1"
   },
   "outputs": [],
   "source": [
    "def extract_and_split_pdfs(pdf_folder_path, chunk_size=400, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Extract text from PDFs in a given folder, then split into chunks using\n",
    "    RecursiveCharacterTextSplitter from langchain.\n",
    "\n",
    "    Args:\n",
    "        pdf_folder_path (str): Path to the folder containing PDF files.\n",
    "        chunk_size (int): Maximum number of characters in each chunk.\n",
    "        chunk_overlap (int): Number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "        all_chunks (List[str]): List of all text chunks from all PDFs.\n",
    "        filenames (List[str]): List of PDF filenames processed.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    filenames = []\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
    "    for filename in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        filenames.append(filename)\n",
    "\n",
    "        # Extract text from the PDF\n",
    "        text = \"\"\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "\n",
    "        # Split text into chunks\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks, filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9934,
     "status": "ok",
     "timestamp": 1731934553782,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "6pyJQjdgnZKC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 509 text chunks total from 5 PDF(s).\n"
     ]
    }
   ],
   "source": [
    "pdf_folder_path = \"pdfs\"\n",
    "chunks, filenames = extract_and_split_pdfs(pdf_folder_path)\n",
    "print(f\"Extracted {len(chunks)} text chunks total from {len(filenames)} PDF(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating and Storing Embeddings of Documents in Qdrant DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 148560,
     "status": "ok",
     "timestamp": 1731934702338,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "1IwlKJ2mnrJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (509, 384)\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = model.encode(chunks).astype(\"float32\")\n",
    "print(\"Embeddings shape:\", document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1731934702339,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "oiJWJ2Lxn2CE",
    "outputId": "d983bad7-bb40-4c3c-e558-17536929dac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'docus_chunks' has been created or recreated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23030014\\AppData\\Local\\Temp\\ipykernel_17460\\2880700545.py:5: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "# Connect to Qdrant (local mode, storing data in 'qdrant_local.db')\n",
    "client = QdrantClient(path=\"local_qdrant.db\")\n",
    "\n",
    "collection_name = \"docus_chunks\"\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=document_embeddings.shape[1],  # Dimension from the embedding model\n",
    "        distance=\"Cosine\"                   # 'Cosine' is typical for sentence embeddings\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' has been created or recreated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731934702339,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "mDsCcXwWn2pd",
    "outputId": "3df0fa1f-d032-49f5-e04f-1c7eb578fde1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 509 chunk embeddings into Qdrant!\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "for i, embedding_vector in enumerate(document_embeddings):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,  # unique ID for each chunk\n",
    "            vector=embedding_vector.tolist(),\n",
    "            payload={\n",
    "                # Store the actual chunk text, or any other metadata you need\n",
    "                \"chunk_text\": chunks[i]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Upsert (insert/update) the chunk embeddings into Qdrant\n",
    "client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "print(f\"Upserted {len(points)} chunk embeddings into Qdrant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9VlIuI8cokK"
   },
   "source": [
    "## **Creating and Storing Embeddings of Q&As in Qdrant DB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Uploading already Generated Q&As**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_qna_from_folder(folder_path):\n",
    "    all_questions = []\n",
    "    all_answers = []\n",
    "\n",
    "    # Iterate through all .txt files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):  # Process only .txt files\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            # Regular expression to extract Q&A pairs\n",
    "            pattern = r'Q:\\s*(.*?)\\s*\\nA:\\s*(.*?)(?=\\s*(?:Q:|$))'\n",
    "            qna_pairs = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "            # Extract questions and answers, preserving their order\n",
    "            all_questions.extend([q.strip() for q, _ in qna_pairs])\n",
    "            all_answers.extend([a.strip() for _, a in qna_pairs])\n",
    "\n",
    "    return all_questions, all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5767 questions and 5767 answers.\n",
      "Q1: Where did the Olympic torch arrive for the first time ever during the 2008 Summer Olympics?\n",
      "A1: Kazakhstan.\n",
      "Q2: Who was the first torchbearer in Almaty, Kazakhstan during the 2008 Summer Olympics?\n",
      "A2: The President of Kazakhstan, Nursultan Nazarbaev.\n",
      "Q3: How many kilometers did the route of the Olympic torch relay run in Almaty?\n",
      "A3: 20 km.\n",
      "Q4: Were there any reports of Uighur activists being arrested during the Olympic torch relay in Kazakhstan?\n",
      "A4: Yes, some were arrested and deported back to China.\n",
      "Q5: When did the Olympic torch arrive in Almaty for the first time ever during the 2008 Summer Olympics?\n",
      "A5: On April 2.\n"
     ]
    }
   ],
   "source": [
    "# Processing\n",
    "folder_path = \"refined_q&as\"\n",
    "questions, answers = extract_qna_from_folder(folder_path)\n",
    "\n",
    "# Verify results\n",
    "print(f\"Extracted {len(questions)} questions and {len(answers)} answers.\")\n",
    "for i in range(min(5, len(questions))):  # Show the first 5 pairs\n",
    "    print(f\"Q{i+1}: {questions[i]}\")\n",
    "    print(f\"A{i+1}: {answers[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Collection in same Local_Qdrant DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731934705417,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "sBAWtJ47J7aj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'docus_chunks' has been created or recreated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Function to create a collection in Qdrant\n",
    "def create_collection(client, collection_name=\"faq_embeddings\"):\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=384, distance=\"Cosine\")  # For all-MiniLM-L6-v2\n",
    "    )\n",
    "\n",
    "print(f\"Collection '{collection_name}' has been created or recreated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCmZju49CL1p"
   },
   "source": [
    "### **Generating Embeddings and Storing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1731934705417,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "ZvZfJsfbLsye"
   },
   "outputs": [],
   "source": [
    "# Function to generate embeddings for a list of questions\n",
    "def generate_embeddings(questions):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(questions)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731934705418,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "0WGvRXI6Kxwn"
   },
   "outputs": [],
   "source": [
    "# Function to store embeddings and metadata (answers) in Qdrant\n",
    "def store_embeddings_in_qdrant(client, collection_name, questions, answers, embeddings):\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=embeddings[i].tolist(),  # Convert numpy array to list\n",
    "            payload={\"answer\": answers[i]}  # Store metadata (only answers)\n",
    "        ) for i in range(len(questions))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591338,
     "status": "ok",
     "timestamp": 1731935296746,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "VY6yedwUBgSH",
    "outputId": "228c07bb-2a7d-4c05-fafb-191692fb0cac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23030014\\AppData\\Local\\Temp\\ipykernel_17460\\1865541097.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "# # Connect to Qdrant in local mode\n",
    "# client = QdrantClient(path=\"local_qdrant.db\")\n",
    "\n",
    "# Create a new collection\n",
    "collection_name = \"faq_embeddings\"\n",
    "create_collection(client, collection_name)\n",
    "\n",
    "# Generate embeddings for questions\n",
    "embeddings = generate_embeddings(questions)\n",
    "\n",
    "# Store embeddings and answers in Qdrant\n",
    "store_embeddings_in_qdrant(client, collection_name, questions, answers, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6EdhQFx3Dag"
   },
   "source": [
    "# **Finding Semantic Similarity of Query with Pdf Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731935296747,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "6PYKapL63JLs"
   },
   "outputs": [],
   "source": [
    "def query_similarity_docus_chunks(client, collection_name, query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for the top_k most similar chunks in a Qdrant collection\n",
    "    that stores 'chunk_text' as payload.\n",
    "    \n",
    "    Args:\n",
    "        client: QdrantClient instance connected to the relevant DB file.\n",
    "        collection_name (str): The name of the collection containing chunk vectors.\n",
    "        query (str): The user query string.\n",
    "        top_k (int): Number of top similar chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        chunks (List[str]): The retrieved chunk texts.\n",
    "        scores (List[float]): The similarity scores.\n",
    "    \"\"\"\n",
    "    # Use the same embedding model you used to store the chunks\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query])[0].tolist()\n",
    "\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Assuming you stored your PDF text in payload[\"chunk_text\"]\n",
    "    chunks = [result.payload[\"chunk_text\"] for result in search_results]\n",
    "    scores = [result.score for result in search_results]\n",
    "\n",
    "    return chunks , scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1968,
     "status": "ok",
     "timestamp": 1731935298708,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "z0nQpTRDUodX",
    "outputId": "a125b419-1127-4440-f937-aea74a57f877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Retrieved Chunks:\n",
      "\n",
      "Chunk 1 (score=0.6124):\n",
      "Cloud\". It is made from aluminum. It is 72 centimetres high and weighs 985 grams. The torch is\n",
      "designed to remain lit in 65 kilometre per hour (37 mile per hour) winds, and in rain of up to 50\n",
      "millimetres (2 inches) per hour. An ignition key is used to ignite and extinguish the flame. The torch\n",
      "is fueled by cans of propane. Each can will light the torch for 15 minutes. It is designed by a team\n",
      "--------------------------------------------------\n",
      "Chunk 2 (score=0.5139):\n",
      "torch of the first torchbearer, a silver medalist of the 2004 Summer Olympics in taekwondo\n",
      "Alexandros Nikolaidis from Greece, who handed the flame over to the second torchbearer, Olympic\n",
      "champion in women\\'s breaststroke Luo Xuejuan from China. Following the recent unrest in Tibet,\n",
      "three members of Reporters Without Borders, including Robert Ménard, breached security and\n",
      "--------------------------------------------------\n",
      "Chunk 3 (score=0.4932):\n",
      "issue, and had its staff in its Chinese stores wear uniforms emblazoned with the Chinese national\n",
      "flag and caps with Olympic insignia and as well as the words \"Beijing 2008\" to show its support for\n",
      "the games. The effort had to be ceased when the BOCOG deemed the use of official Olympic\n",
      "insignia as illegal and a violation of copyright. The torch was lit at a park outside at AT&T Park at\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc_collection_name = \"docus_chunks\" \n",
    "query = \"What is the Olympic Torch made from?\"\n",
    "\n",
    "chunks, chunk_scores = query_similarity_docus_chunks(client, doc_collection_name, query, top_k=3)\n",
    "\n",
    "print(\"Top 3 Retrieved Chunks:\\n\")\n",
    "for i, (chunk, score) in enumerate(zip(chunks, chunk_scores), 1):\n",
    "    print(f\"Chunk {i} (score={score:.4f}):\\n{chunk}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-2A7gRABlER"
   },
   "source": [
    "# **Finding Semantic Similarity of Query with Q&As**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731935298708,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "gP3sPs1rGRHo"
   },
   "outputs": [],
   "source": [
    "# Function to query Qdrant for top-k similar embeddings\n",
    "def query_similar_embeddings(client, collection_name, query, top_k=5):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    query_embedding = model.encode([query])[0].tolist()  # Generate query embedding\n",
    "\n",
    "    # Perform the search to get the most similar vectors\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k  # Number of nearest neighbors to retrieve\n",
    "    )\n",
    "\n",
    "    # Retrieve answers from the search results\n",
    "    answers = [result.payload['answer'] for result in search_results]\n",
    "    scores = [result.score for result in search_results]\n",
    "\n",
    "    return answers , scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 642,
     "status": "ok",
     "timestamp": 1731935299345,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "-8wysrbDFOoh",
    "outputId": "2cf75cc7-1866-42fc-f6eb-403f16d5521d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Answers\n",
      "1: Traditional scrolls.\n",
      "2: Aluminum.\n",
      "3: No, it is used during various events and ceremonies.\n",
      "4: It is 72 centimetres high.\n",
      "5: Macau.\n"
     ]
    }
   ],
   "source": [
    "# Query similar questions to a given query\n",
    "query = \"What is the Olympic Torch made from?\"\n",
    "\n",
    "retrieved_answers , scores = query_similar_embeddings(client, collection_name, query)\n",
    "\n",
    "\n",
    "# Display retrieved answers\n",
    "print(\"Retrieved Answers\")\n",
    "for i, answer in enumerate(retrieved_answers, 1):\n",
    "    print(f\"{i}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1731935299345,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "iDt4VMd1TMbv",
    "outputId": "e04fc608-6f0f-41c8-e8c1-e46a7abea129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Scores\n",
      "1: 0.8671069641571898\n",
      "2: 0.7382860309141743\n",
      "3: 0.7291142660523945\n",
      "4: 0.7124895793365728\n",
      "5: 0.7088016914154129\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved Scores\")\n",
    "for i, answer in enumerate(scores, 1):\n",
    "    print(f\"{i}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzBucbLZSDsP"
   },
   "source": [
    "### **Generating response from Retrived_Answers using Ollama Llama3 ChatQA 8b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_answer_using_ollama(\n",
    "    retrieved_context: str,\n",
    "    query: str,\n",
    "    base_url: str = \"http://localhost:11434/v1\",\n",
    "    api_key: str = \"ollama\",\n",
    "    model: str = \"llama3-chatqa:8b\"\n",
    ") -> str:\n",
    "    prompt = (\n",
    "        f\"Query: {query}\\n\"\n",
    "        f\"Context (answers from the most relevant context/ sentences related to this query): {retrieved_context}\\n\\n\"\n",
    "        f\"Answer the query using only the provided context. Provide a concise, direct answer. If no relevant information is available, please indicate so.\"\n",
    "    )\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.3,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Sending request to Ollama...\")\n",
    "        response = requests.post(\n",
    "            f\"{base_url}/completions\",\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            json=payload,\n",
    "            timeout=60\n",
    "        )\n",
    "        logger.info(\"Received response from Ollama\")\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if data.get(\"choices\"):\n",
    "            return data[\"choices\"][0][\"text\"].strip()\n",
    "        return \"No response generated.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating answer: {e}\")\n",
    "        return \"An error occurred while generating the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:03:12,843 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:04:16,327 - INFO - Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: Aluminum\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Olympic Torch made from?\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Generate an answer using the local Ollama server\n",
    "    final_answer = generate_answer_using_ollama(retrieved_answers, query)\n",
    "    print(\"Generated Answer:\", final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSayKTfb3zQm"
   },
   "source": [
    "# **RAG Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1731939581312,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "jWkp7Zvy3coo"
   },
   "outputs": [],
   "source": [
    "def traditional_rag_pipeline(client, trad_collection_name, query):\n",
    "    \"\"\"\n",
    "    Executes the traditional RAG approach.\n",
    "    Returns results, retrieval time, and generation time.\n",
    "    \"\"\"\n",
    "    start_retrieval = time.time()\n",
    "    retrieved_context , similarity_scores = query_similarity_docus_chunks(client, trad_collection_name, query, top_k=3)  # Retrieve relevant sentences\n",
    "    end_retrieval = time.time()\n",
    "    retrieval_time = end_retrieval - start_retrieval\n",
    "\n",
    "    start_answer = time.time()\n",
    "    generated_answer = generate_answer_using_ollama(retrieved_context, query)  # Generate answer\n",
    "    end_answer = time.time()\n",
    "    generation_time = end_answer - start_answer\n",
    "\n",
    "    total_time = retrieval_time + generation_time\n",
    "\n",
    "    result = {\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieval_time\": retrieval_time,\n",
    "        \"answer\": generated_answer,  # Make sure this key is correctly returned\n",
    "        \"generation_time\": generation_time,\n",
    "        \"total_time\": total_time,\n",
    "        \"similarity score\": similarity_scores\n",
    "    }\n",
    "\n",
    "    # Debug: Print the result to inspect the return value\n",
    "    print(\"Traditional RAG Result Generated\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731935301325,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "kIV8-R7F3xDr"
   },
   "outputs": [],
   "source": [
    "def mod_rag_pipeline(query, client, mod_collection_name, top_k=3):\n",
    "    \"\"\"\n",
    "    Executes the second RAG approach using query embeddings.\n",
    "    Returns results, retrieval time, and other metadata.\n",
    "    \"\"\"\n",
    "    start_retrieval = time.time()\n",
    "    answers, scores = query_similar_embeddings(client, mod_collection_name, query, top_k)  # Now returns two lists\n",
    "    end_retrieval = time.time()\n",
    "    retrieval_time = end_retrieval - start_retrieval\n",
    "\n",
    "    if answers:\n",
    "        # Combine answers with their corresponding similarity scores for context\n",
    "        retrieved_documents = [f\"Answer: {answers[i]}\\nScore: {scores[i]:.2f}\" for i in range(len(answers))]\n",
    "\n",
    "        # Generate answer based on retrieved answers\n",
    "        start_answer = time.time()\n",
    "        generated_answer = generate_answer_using_ollama(answers, query)\n",
    "        end_answer = time.time()\n",
    "        answer_time = end_answer - start_answer\n",
    "\n",
    "        total_time = retrieval_time + answer_time\n",
    "\n",
    "        return {\n",
    "            \"retrieved_documents\": retrieved_documents,\n",
    "            \"similarity_scores\": scores,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"generation_time\": answer_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"retrieved_documents\": [],\n",
    "            \"similarity_scores\": [],\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generated_answer\": None,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": retrieval_time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731935301325,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "YO3htCRoGwgx"
   },
   "outputs": [],
   "source": [
    "def rag_pipeline(query, client, trad_collection_name, mod_collection_name, similarity_threshold=0.8, top_k=3):\n",
    "    \"\"\"\n",
    "    Main pipeline that first tries query embeddings (approach 2).\n",
    "    Falls back to traditional RAG if similarity scores are below the threshold.\n",
    "    \"\"\"\n",
    "    # Step 1: Try query embeddings (Approach 2)\n",
    "    query_similar_results = mod_rag_pipeline(query, client, mod_collection_name, top_k)\n",
    "\n",
    "    # Check similarity scores\n",
    "    if query_similar_results[\"similarity_scores\"]:\n",
    "        max_similarity = max(query_similar_results[\"similarity_scores\"])\n",
    "\n",
    "        if max_similarity >= similarity_threshold:\n",
    "            # Return results from the query embeddings approach\n",
    "            return {\n",
    "                \"case\": \"Q-A Index\",\n",
    "                \"answer\": query_similar_results[\"generated_answer\"],\n",
    "                \"retrieved_context\": \"\\n\".join(query_similar_results[\"retrieved_documents\"]),\n",
    "                \"retrieval_time\": query_similar_results[\"retrieval_time\"],\n",
    "                \"generated_context\": query_similar_results[\"generated_answer\"],\n",
    "                \"generation_time\": query_similar_results[\"generation_time\"],\n",
    "                \"total_time\": query_similar_results[\"total_time\"],\n",
    "            }\n",
    "\n",
    "    # Step 2: Fallback to traditional RAG if similarity is low or no results\n",
    "    traditional_results = traditional_rag_pipeline(client, trad_collection_name, query)\n",
    "\n",
    "    return {\n",
    "        \"case\": \"Traditional RAG\",\n",
    "        \"answer\": traditional_results[\"answer\"],\n",
    "        \"retrieved_context\": \"\\n\".join(traditional_results[\"retrieved_context\"]),\n",
    "        \"retrieval_time\": traditional_results[\"retrieval_time\"],\n",
    "        \"generation_time\": traditional_results[\"generation_time\"],\n",
    "        \"total_time\": traditional_results[\"total_time\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3505,
     "status": "ok",
     "timestamp": 1731935468319,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "fClO9Bp4VM6N",
    "outputId": "4ca37ea5-3308-440c-eefa-509322a87968"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:04:16,375 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:04:16,375 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.22it/s]\n",
      "2025-04-12 15:04:19,042 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:05:22,581 - INFO - Received response from Ollama\n",
      "2025-04-12 15:05:22,583 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:05:22,583 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 213.97it/s]\n",
      "2025-04-12 15:05:25,298 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:06:29,432 - INFO - Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Case: Traditional RAG\n",
      "\n",
      "Answer: The ceremony was held in Beijing, China.\n",
      "\n",
      "Retrieved Context: 2008_Summer_Olympics_torch_relay\n",
      "done peacefully.\". Also, Australia\\'s ACT Chief Minister, Jon Stanhope confirmed that the Chinese\n",
      "embassy was closely involve to ensure that \"pro-China demonstrators vastly outnumbered Tibetan\n",
      "activists.\" Australian freestyle swimmer and five-time Olympic gold medalist Ian Thorpe ended the\n",
      "Olympic flame passed next to were the Parliament House, National Mosque, KL Tower and\n",
      "Merdeka Stadium. A team of 1000 personnel from the Malaysian police Special Action Squad\n",
      "guarded the event and escorted the torchbearers. The last time an Olympic torch relay was held in\n",
      "2008_Summer_Olympics_torch_relay\n",
      "2008_Summer_Olympics_torch_relay\n",
      "San Francisco\\'s Marina district and was then moved by bus to San Francisco International Airport\n",
      "for a makeshift closing ceremony at the terminal, from which the free media was excluded. San Jose\n",
      "Mercury News described the \"deceiving\" event as \"a game of Where\\'s Waldo, played against the\n",
      "\n",
      "Retrieval Time: 2.72s\n",
      "\n",
      "Generation Time: 64.13s\n",
      "\n",
      "Total Time: 66.85s\n"
     ]
    }
   ],
   "source": [
    "# Execute the RAG pipeline\n",
    "results = rag_pipeline(query=\"Where was ceremony of Olympics 2008 held?\", client=client, trad_collection_name=\"docus_chunks\", mod_collection_name=\"faq_embeddings\", similarity_threshold=0.8, top_k=3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Case: {results['case']}\\n\")\n",
    "print(f\"Answer: {results['answer']}\\n\")\n",
    "print(f\"Retrieved Context: {results['retrieved_context']}\\n\")\n",
    "print(f\"Retrieval Time: {results['retrieval_time']:.2f}s\\n\")\n",
    "print(f\"Generation Time: {results['generation_time']:.2f}s\\n\")\n",
    "print(f\"Total Time: {results['total_time']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Ground Truth CSVs and merging results into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame (is_possible != True):\n",
      "                               title  \\\n",
      "0   2008_Summer_Olympics_torch_relay   \n",
      "1   2008_Summer_Olympics_torch_relay   \n",
      "2                                NaN   \n",
      "3                                NaN   \n",
      "4                                NaN   \n",
      "..                               ...   \n",
      "89                               NaN   \n",
      "90                               NaN   \n",
      "91                               NaN   \n",
      "92                               NaN   \n",
      "93                               NaN   \n",
      "\n",
      "                                              context  \\\n",
      "0   ['The 2008 Summer Olympics torch relay was run...   \n",
      "1   ['The 2008 Summer Olympics torch relay was run...   \n",
      "2                                                 NaN   \n",
      "3                                                 NaN   \n",
      "4                                                 NaN   \n",
      "..                                                ...   \n",
      "89                                                NaN   \n",
      "90                                                NaN   \n",
      "91                                                NaN   \n",
      "92                                                NaN   \n",
      "93                                                NaN   \n",
      "\n",
      "                                             question                answers  \\\n",
      "0   When did the tradition of people carrying the ...  1936 Summer Olympics.   \n",
      "1   How many days did people carry the Olympic tor...               129 days   \n",
      "2                                                 NaN                    NaN   \n",
      "3                                                 NaN                    NaN   \n",
      "4                                                 NaN                    NaN   \n",
      "..                                                ...                    ...   \n",
      "89                                                NaN                    NaN   \n",
      "90                                                NaN                    NaN   \n",
      "91                                                NaN                    NaN   \n",
      "92                                                NaN                    NaN   \n",
      "93                                                NaN                    NaN   \n",
      "\n",
      "   is_impossible  \n",
      "0          False  \n",
      "1          False  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "..           ...  \n",
      "89           NaN  \n",
      "90           NaN  \n",
      "91           NaN  \n",
      "92           NaN  \n",
      "93           NaN  \n",
      "\n",
      "[94 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_matching_csv_files(filenames, csv_folder_path):\n",
    "    # Remove .pdf extension from each filename in the list\n",
    "    filenames = [os.path.splitext(f)[0] for f in filenames]\n",
    "\n",
    "    # Filter for CSV files that match the filenames list\n",
    "    matching_csv_files = [\n",
    "        os.path.join(csv_folder_path, f) for f in os.listdir(csv_folder_path)\n",
    "        if f.endswith('.csv') and os.path.splitext(f)[0] in filenames\n",
    "    ]\n",
    "\n",
    "    return matching_csv_files\n",
    "\n",
    "def merge_csv_files(file_paths):\n",
    "    # Read and concatenate all CSV files into a single DataFrame\n",
    "    dataframes = [pd.read_csv(file) for file in file_paths]\n",
    "    merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_folder_path = 'Ground Truths'\n",
    "\n",
    "# Get the list of matching CSV files\n",
    "matching_csv_files = get_matching_csv_files(filenames, csv_folder_path)\n",
    "\n",
    "# Merge all matching CSV files into one DataFrame\n",
    "merged_df = merge_csv_files(matching_csv_files)\n",
    "\n",
    "# Initialize filtered_df to merged_df to ensure it's defined\n",
    "filtered_df = merged_df\n",
    "\n",
    "# Remove rows where the 'is_possible' column has a value of True\n",
    "if 'is_possible' in merged_df.columns:\n",
    "    filtered_df = merged_df[~merged_df['is_possible']]\n",
    "\n",
    "# Display the filtered DataFrame or save it to a new CSV file\n",
    "print(\"Filtered DataFrame (is_possible != True):\")\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answer_columns(filtered_df):\n",
    "    # Select only 'question' and 'answers' columns from the merged DataFrame\n",
    "    if 'question' in merged_df.columns and 'answers' in merged_df.columns:\n",
    "        return merged_df[['question', 'answers']]\n",
    "    else:\n",
    "        raise ValueError(\"The columns 'question' and/or 'answers' do not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('output_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_path = \"output_test.csv\"\n",
    "merged_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=get_question_answer_columns(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did the tradition of people carrying the ...</td>\n",
       "      <td>1936 Summer Olympics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many days did people carry the Olympic tor...</td>\n",
       "      <td>129 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question                answers\n",
       "0   When did the tradition of people carrying the ...  1936 Summer Olympics.\n",
       "1   How many days did people carry the Olympic tor...               129 days\n",
       "2                                                 NaN                    NaN\n",
       "3                                                 NaN                    NaN\n",
       "4                                                 NaN                    NaN\n",
       "..                                                ...                    ...\n",
       "89                                                NaN                    NaN\n",
       "90                                                NaN                    NaN\n",
       "91                                                NaN                    NaN\n",
       "92                                                NaN                    NaN\n",
       "93                                                NaN                    NaN\n",
       "\n",
       "[94 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question                answers\n",
      "0  When did the tradition of people carrying the ...  1936 Summer Olympics.\n",
      "1  How many days did people carry the Olympic tor...               129 days\n",
      "2  Along with pop, soul, rhythm and blues, quiet ...         easy listening\n",
      "3  What was the prevailing style of adult contemp...              soft rock\n",
      "4  Outside of employment, what is the other main ...              education\n",
      "5  Affirmative action attempts to ask institution...      racial minorities\n",
      "6               What type of carrier is the largest?          fleet carrier\n",
      "7        What capability does a fleet carrier offer?              offensive\n",
      "8  What is an aerodome with facilities for flight...                airport\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NaN in either 'question' or 'answers' column\n",
    "new_df = new_df.dropna(subset=['question', 'answers']).reset_index(drop=True)\n",
    "\n",
    "# View the cleaned DataFrame\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating results for all Ground Truth Q&As**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA available? True\n",
      "GPU name: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch CUDA available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Answers Generate** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:14:14,398 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:14:14,399 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.07it/s]\n",
      "2025-04-12 15:14:21,468 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:15:26,194 - INFO - Received response from Ollama\n",
      "2025-04-12 15:15:26,196 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:15:26,198 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-04-12 15:15:29,454 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:16:33,886 - INFO - Received response from Ollama\n",
      "2025-04-12 15:16:33,888 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:16:33,888 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.93it/s]\n",
      "2025-04-12 15:16:36,535 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:17:41,011 - INFO - Received response from Ollama\n",
      "2025-04-12 15:17:41,012 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:17:41,012 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 1 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-04-12 15:17:43,724 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:17:46,290 - INFO - Received response from Ollama\n",
      "2025-04-12 15:17:46,292 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:17:46,292 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-04-12 15:17:49,305 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:18:53,795 - INFO - Received response from Ollama\n",
      "2025-04-12 15:18:53,800 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:18:53,801 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 2 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 713.20it/s]\n",
      "2025-04-12 15:18:56,692 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:20:00,710 - INFO - Received response from Ollama\n",
      "2025-04-12 15:20:00,714 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:20:00,714 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-04-12 15:20:03,627 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:21:07,850 - INFO - Received response from Ollama\n",
      "2025-04-12 15:21:07,851 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:21:07,851 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 126.22it/s]\n",
      "2025-04-12 15:21:10,464 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:21:17,417 - INFO - Received response from Ollama\n",
      "2025-04-12 15:21:17,418 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:21:17,418 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 3 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.05it/s]\n",
      "2025-04-12 15:21:20,798 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:22:26,802 - INFO - Received response from Ollama\n",
      "2025-04-12 15:22:26,805 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:22:26,806 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.43it/s]\n",
      "2025-04-12 15:22:30,228 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:23:37,118 - INFO - Received response from Ollama\n",
      "2025-04-12 15:23:37,124 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:23:37,125 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 4 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.69it/s]\n",
      "2025-04-12 15:23:40,106 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:24:44,211 - INFO - Received response from Ollama\n",
      "2025-04-12 15:24:44,213 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:24:44,214 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 143.28it/s]\n",
      "2025-04-12 15:24:46,937 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:25:51,235 - INFO - Received response from Ollama\n",
      "2025-04-12 15:25:51,241 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:25:51,242 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 5 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "2025-04-12 15:25:54,049 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:26:00,349 - INFO - Received response from Ollama\n",
      "2025-04-12 15:26:00,351 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:26:00,352 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 129.94it/s]\n",
      "2025-04-12 15:26:03,081 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:27:10,971 - INFO - Received response from Ollama\n",
      "2025-04-12 15:27:10,975 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:27:10,975 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.84it/s]\n",
      "2025-04-12 15:27:13,622 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:28:18,032 - INFO - Received response from Ollama\n",
      "2025-04-12 15:28:18,033 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:28:18,033 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 6 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.11it/s]\n",
      "2025-04-12 15:28:21,173 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:29:25,355 - INFO - Received response from Ollama\n",
      "2025-04-12 15:29:25,357 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:29:25,357 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-04-12 15:29:28,303 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:30:33,647 - INFO - Received response from Ollama\n",
      "2025-04-12 15:30:33,650 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:30:33,650 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 7 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.01it/s]\n",
      "2025-04-12 15:30:36,807 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:30:41,653 - INFO - Received response from Ollama\n",
      "2025-04-12 15:30:41,654 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:30:41,654 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "2025-04-12 15:30:44,273 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:31:49,902 - INFO - Received response from Ollama\n",
      "2025-04-12 15:31:49,908 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:31:49,908 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 8 queries. Saved partial results to Final_Answers_Generated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 125.34it/s]\n",
      "2025-04-12 15:31:52,581 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:31:56,565 - INFO - Received response from Ollama\n",
      "2025-04-12 15:31:56,566 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:31:56,566 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 128.17it/s]\n",
      "2025-04-12 15:31:59,791 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:33:05,541 - INFO - Received response from Ollama\n",
      "2025-04-12 15:33:05,542 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-12 15:33:05,542 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 1089.43it/s]\n",
      "2025-04-12 15:33:08,223 - INFO - Sending request to Ollama...\n",
      "2025-04-12 15:34:14,134 - INFO - Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 9 queries. Saved partial results to Final_Answers_Generated.csv\n",
      "All queries processed. Final results saved to: Final_Answers_Generated.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty columns in new_df. \n",
    "# If they don't exist, you can create them to ensure they are present before the loop.\n",
    "new_df['modified_rag_retrieved_context'] = \"\"\n",
    "new_df['modified_rag_refined_answer'] = \"\"\n",
    "new_df['modified_rag_case'] = \"\"\n",
    "new_df['modified_rag_retrieval_time'] = \"\"\n",
    "new_df['modified_rag_generation_time'] = \"\"\n",
    "new_df['modified_rag_total_time'] = \"\"\n",
    "\n",
    "new_df['traditional_rag_retrieved_context'] = \"\"\n",
    "new_df['traditional_rag_refined_answer'] = \"\"\n",
    "new_df['traditional_rag_retrieval_time'] = \"\"\n",
    "new_df['traditional_rag_generation_time'] = \"\"\n",
    "new_df['traditional_rag_total_time'] = \"\"\n",
    "\n",
    "output_file = 'Final_Answers_Generated.csv'\n",
    "\n",
    "it = 0\n",
    "for index, row in new_df.iterrows():\n",
    "    query = row['question']  # Extract the question from the CSV\n",
    "\n",
    "    # Pass the query through the RAG pipeline\n",
    "    rag_result = rag_pipeline(\n",
    "        query, \n",
    "        client=client, \n",
    "        trad_collection_name=\"docus_chunks\", \n",
    "        mod_collection_name=\"faq_embeddings\", \n",
    "        similarity_threshold=0.8, \n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    # Extract RAG results\n",
    "    retrival_time = rag_result[\"retrieval_time\"]\n",
    "    generation_time = rag_result[\"generation_time\"]\n",
    "    total_time = rag_result[\"total_time\"]\n",
    "    retrieved_context = rag_result[\"retrieved_context\"]\n",
    "    refined_answer = rag_result[\"answer\"]\n",
    "    case = rag_result[\"case\"]\n",
    "\n",
    "    # Update the DataFrame directly for the current row\n",
    "    new_df.at[index, 'modified_rag_retrieved_context'] = retrieved_context\n",
    "    new_df.at[index, 'modified_rag_refined_answer'] = refined_answer\n",
    "    new_df.at[index, 'modified_rag_case'] = case\n",
    "    new_df.at[index, 'modified_rag_retrieval_time'] = f\"{retrival_time:.2f}\"\n",
    "    new_df.at[index, 'modified_rag_generation_time'] = f\"{generation_time:.2f}\"\n",
    "    new_df.at[index, 'modified_rag_total_time'] = f\"{total_time:.2f}\"\n",
    "\n",
    "    # Traditional RAG approach\n",
    "    traditional_results = traditional_rag_pipeline(client, \"docus_chunks\", query)\n",
    "    trad_rag_retrieved_context_str = \"\\n\\n\".join(traditional_results[\"retrieved_context\"])\n",
    "    trad_refined_answer = traditional_results[\"answer\"]\n",
    "    trad_retrieval_time = traditional_results[\"retrieval_time\"]\n",
    "    trad_generation_time = traditional_results[\"generation_time\"]\n",
    "    trad_total_time = traditional_results[\"total_time\"]\n",
    "\n",
    "    # Update DataFrame columns for traditional RAG\n",
    "    new_df.at[index, 'traditional_rag_retrieved_context'] = trad_rag_retrieved_context_str\n",
    "    new_df.at[index, 'traditional_rag_refined_answer'] = trad_refined_answer\n",
    "    new_df.at[index, 'traditional_rag_retrieval_time'] = f\"{trad_retrieval_time:.2f}\"\n",
    "    new_df.at[index, 'traditional_rag_generation_time'] = f\"{trad_generation_time:.2f}\"\n",
    "    new_df.at[index, 'traditional_rag_total_time'] = f\"{trad_total_time:.2f}\"\n",
    "\n",
    "    # After each iteration, save the partially updated DataFrame to CSV\n",
    "    # You could also use a different file name for partial results, if you prefer.\n",
    "    new_df.to_csv(output_file, index=False)\n",
    "\n",
    "    it += 1\n",
    "    print(f\"Processed {it} queries. Saved partial results to {output_file}\")\n",
    "\n",
    "print(\"All queries processed. Final results saved to:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1PoJ2eXvfpybgZMYLuXRB2VodilOLSIAC",
     "timestamp": 1731917087961
    },
    {
     "file_id": "1lt7bRC58P4uVjLE4loNBBFxofRbcZhP0",
     "timestamp": 1731911135884
    }
   ]
  },
  "kernelspec": {
   "display_name": "myenv11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
