{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.9/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.9/site-packages (2.7.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pdfminer.six in ./venv/lib/python3.9/site-packages (20250506)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.9/site-packages (4.1.0)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting qdrant-client\n",
      "  Using cached qdrant_client-1.14.2-py3-none-any.whl (327 kB)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.9/site-packages (1.79.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/lib/python3.9/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./venv/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.9/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./venv/lib/python3.9/site-packages (from pdfminer.six) (45.0.2)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./venv/lib/python3.9/site-packages (from pdfminer.six) (3.4.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.9/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./venv/lib/python3.9/site-packages (from langchain) (2.11.4)\n",
      "Collecting langsmith<0.4,>=0.1.17\n",
      "  Downloading langsmith-0.3.42-py3-none-any.whl (360 kB)\n",
      "\u001b[K     |████████████████████████████████| 360 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.41-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.58\n",
      "  Downloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
      "\u001b[K     |████████████████████████████████| 437 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: httpx[http2]>=0.20.0 in ./venv/lib/python3.9/site-packages (from qdrant-client) (0.28.1)\n",
      "Collecting grpcio>=1.41.0\n",
      "  Downloading grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.20.0\n",
      "  Using cached protobuf-6.31.0-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in ./venv/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.9/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Collecting h2<5,>=3\n",
      "  Downloading h2-4.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hpack<5,>=4.1\n",
      "  Downloading hpack-4.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting hyperframe<7,>=6.1\n",
      "  Downloading hyperframe-6.1.0-py3-none-any.whl (13 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp39-cp39-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[K     |████████████████████████████████| 633 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "\u001b[K     |████████████████████████████████| 249 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 733 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Installing collected packages: zstandard, requests-toolbelt, packaging, orjson, jsonpointer, tenacity, langsmith, jsonpatch, hyperframe, hpack, langchain-core, h2, SQLAlchemy, protobuf, portalocker, langchain-text-splitters, grpcio, async-timeout, sentencepiece, qdrant-client, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "Successfully installed SQLAlchemy-2.0.41 async-timeout-4.0.3 grpcio-1.71.0 h2-4.2.0 hpack-4.1.0 hyperframe-6.1.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.25 langchain-core-0.3.60 langchain-text-splitters-0.3.8 langsmith-0.3.42 orjson-3.10.18 packaging-24.2 portalocker-2.10.1 protobuf-6.31.0 qdrant-client-1.14.2 requests-toolbelt-1.0.0 sentencepiece-0.2.0 tenacity-9.1.2 zstandard-0.23.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/mac/Documents/LMA-RAG Code/LMA-RAG Thesis Code/OpenAI FAQ/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch sentencepiece pdfminer.six numpy pandas tqdm requests sentence-transformers langchain qdrant-client openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1731934540989,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "PHoU2URCon7J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Documents/LMA-RAG Code/LMA-RAG Thesis Code/OpenAI FAQ/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mac/Documents/LMA-RAG Code/LMA-RAG Thesis Code/OpenAI FAQ/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, PointStruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM-8sNJbnO6r"
   },
   "source": [
    "### **Processing Pdfs Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731934541356,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "3Y5WGWOVnHy1"
   },
   "outputs": [],
   "source": [
    "def extract_and_split_pdfs(pdf_folder_path, chunk_size=400, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Extract text from PDFs using pdfminer and split using LangChain's splitter.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"۔\", \"\\n\", \",\", \" \"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    filenames = []\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
    "    for filename in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        filenames.append(filename)\n",
    "\n",
    "        # Extract text using pdfminer\n",
    "        try:\n",
    "            text = extract_text(pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        chunks = splitter.split_text(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    return all_chunks, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9934,
     "status": "ok",
     "timestamp": 1731934553782,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "6pyJQjdgnZKC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 635 text chunks total from 2 PDF(s).\n"
     ]
    }
   ],
   "source": [
    "pdf_folder_path = \"urdu_pdfs\"\n",
    "chunks, filenames = extract_and_split_pdfs(pdf_folder_path)\n",
    "print(f\"Extracted {len(chunks)} text chunks total from {len(filenames)} PDF(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008\\n\\n2008\\n\\n2008\\n\\n ﮐو ، ﺳرﮐﺎری ﻋﮩدﯾداروں ﻧﮯ ﮨزاروں ﮔرے ﮨوﺋﮯ اﺳﮑوﻟوں ﮐﮯ ﮐﮭﻧڈرات ﮐﺎ ﻣﻌﺎﺋﻧہ ﮐرﻧﺎ ﺷروع ﮐﯾﺎ ، اس \\n\\n ﮐو ﮔﻠوب اﯾﻧڈ ﻣﯾل ڈاٹ ﮐﺎم ﮐﮯ ﺟﯾﻔری ﯾﺎرک ﻧﮯ اطﻼع دی ﮐہ ﺧراب طرﯾﻘﮯ ﺳﮯ ﺗﻌﻣﯾر ﺷده ﻋﻣﺎرﺗوں ﮐو \\n\\n29 ﻣﺋﯽ \\nﺑﺎرے ﻣﯾں اﺷﺎرے ﺗﻼش ﮐرﻧﮯ ﮐﮯ ﻟﺋﮯ ﮐہ وه ﮐﯾوں ﮔر ﮔﺋﮯ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating and Storing Embeddings of Documents in Qdrant DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 148560,
     "status": "ok",
     "timestamp": 1731934702338,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "1IwlKJ2mnrJ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (635, 1024)\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = model.encode(chunks).astype(\"float32\")\n",
    "print(\"Embeddings shape:\", document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1731934702339,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "oiJWJ2Lxn2CE",
    "outputId": "d983bad7-bb40-4c3c-e558-17536929dac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'docus_chunks' has been created or recreated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/3578824758.py:4: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = QdrantClient(path=\"new_local_qdrant_vectordb.db\")\n",
    "\n",
    "collection_name = \"docus_chunks\"\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=document_embeddings.shape[1],  # Dimension from the embedding model\n",
    "        distance=\"Cosine\"                   # 'Cosine' is typical for sentence embeddings\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' has been created or recreated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731934702339,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "mDsCcXwWn2pd",
    "outputId": "3df0fa1f-d032-49f5-e04f-1c7eb578fde1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 635 chunk embeddings into Qdrant!\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "for i, embedding_vector in enumerate(document_embeddings):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,  # unique ID for each chunk\n",
    "            vector=embedding_vector.tolist(),\n",
    "            payload={\n",
    "                # Store the actual chunk text, or any other metadata you need\n",
    "                \"chunk_text\": chunks[i]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Upsert (insert/update) the chunk embeddings into Qdrant\n",
    "client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "print(f\"Upserted {len(points)} chunk embeddings into Qdrant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9VlIuI8cokK"
   },
   "source": [
    "## **Creating and Storing Embeddings of Q&As in Qdrant DB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Uploading already Generated Q&As**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_urdu_qa_pairs(csv_path):\n",
    "    \"\"\"\n",
    "    Loads Urdu QA pairs from a CSV file and returns two lists: questions and answers.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (questions, answers), where both are lists of strings.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Ensure the required columns exist\n",
    "    if 'question' not in df.columns or 'answer' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'question' and 'answer' columns.\")\n",
    "\n",
    "    questions = df['question'].dropna().tolist()\n",
    "    answers = df['answer'].dropna().tolist()\n",
    "\n",
    "    return questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29 مئی 2008 کو سرکاری عہدیداروں نے کیا اقدام کیا؟', 'والدین نے مقامی عہدیداروں اور بلڈرز پر کیا الزام لگایا؟', 'والدین نے زلزلے کے بعد دیگر عمارتوں کے بارے میں کیا کہا؟', 'زلزلے کے بعد سرکاری طور پر کیا وعدہ کیا گیا؟', '17 جولائی 2008 تک والدین کی کیا شکایت تھی؟']\n",
      "['سرکاری عہدیداروں نے ہزاروں گرے ہوئے اسکولوں کے کھنڈرات کا معائنہ کرنا شروع کیا۔', 'والدین نے الزام لگایا کہ اسکول کی تعمیر میں کونوں کونوں کاٹنے کی وجہ سے اسکول گرے۔', 'والدین نے کہا کہ زلزلے کے بعد قریبی دیگر عمارتوں کو بہت کم نقصان پہنچا تھا۔', 'زلزلے کے بعد بہت سی مقامی حکومتوں نے سرکاری طور پر اسکول کے گرنے کی تحقیقات کرنے کا وعدہ کیا۔', 'والدین نے شکایت کی کہ انہیں ابھی تک کوئی رپورٹ موصول نہیں ہوئی۔']\n"
     ]
    }
   ],
   "source": [
    "questions, answers = load_urdu_qa_pairs(\"urdu_qa_pairs_updated_openai_4o.csv\")\n",
    "print(questions[:5])\n",
    "print(answers[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Collection in same Local_Qdrant DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731934705417,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "sBAWtJ47J7aj"
   },
   "outputs": [],
   "source": [
    "# Function to create a collection in Qdrant\n",
    "def create_collection(client, collection_name=\"faq_embeddings\"):\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=1024, distance=\"Cosine\")  # For all-MiniLM-L6-v2\n",
    "    )\n",
    "\n",
    "    print(f\"Collection '{collection_name}' has been created or recreated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCmZju49CL1p"
   },
   "source": [
    "### **Generating Embeddings and Storing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1731934705417,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "ZvZfJsfbLsye"
   },
   "outputs": [],
   "source": [
    "# Function to generate embeddings for a list of questions\n",
    "def generate_embeddings(questions):\n",
    "    model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "    embeddings = model.encode(questions)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731934705418,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "0WGvRXI6Kxwn"
   },
   "outputs": [],
   "source": [
    "# Function to store embeddings and metadata (answers) in Qdrant\n",
    "def store_embeddings_in_qdrant(client, collection_name, questions, answers, embeddings):\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=embeddings[i].tolist(),  # Convert numpy array to list\n",
    "            payload={\"answer\": answers[i]}  # Store metadata (only answers)\n",
    "        ) for i in range(len(questions))\n",
    "    ]\n",
    "    client.upsert(collection_name=collection_name, points=points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591338,
     "status": "ok",
     "timestamp": 1731935296746,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "VY6yedwUBgSH",
    "outputId": "228c07bb-2a7d-4c05-fafb-191692fb0cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'faq_embeddings' has been created or recreated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/3904413588.py:3: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collection_name = \"faq_embeddings\"\n",
    "create_collection(client, collection_name)\n",
    "\n",
    "# Generate embeddings for questions\n",
    "embeddings = generate_embeddings(questions)\n",
    "\n",
    "# Store embeddings and answers in Qdrant\n",
    "store_embeddings_in_qdrant(client, collection_name, questions, answers, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6EdhQFx3Dag"
   },
   "source": [
    "# **Finding Semantic Similarity of Query with Pdf Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731935296747,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "6PYKapL63JLs"
   },
   "outputs": [],
   "source": [
    "def query_similarity_docus_chunks(client, collection_name, query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for the top_k most similar chunks in a Qdrant collection\n",
    "    that stores 'chunk_text' as payload.\n",
    "    \n",
    "    Args:\n",
    "        client: QdrantClient instance connected to the relevant DB file.\n",
    "        collection_name (str): The name of the collection containing chunk vectors.\n",
    "        query (str): The user query string.\n",
    "        top_k (int): Number of top similar chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        chunks (List[str]): The retrieved chunk texts.\n",
    "        scores (List[float]): The similarity scores.\n",
    "    \"\"\"\n",
    "    # Use the same embedding model you used to store the chunks\n",
    "\n",
    "\n",
    "    model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "    \n",
    "    query_embedding = model.encode([query])[0].tolist()\n",
    "\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Assuming you stored your PDF text in payload[\"chunk_text\"]\n",
    "    chunks = [result.payload[\"chunk_text\"] for result in search_results]\n",
    "    scores = [result.score for result in search_results]\n",
    "\n",
    "    return chunks , scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1968,
     "status": "ok",
     "timestamp": 1731935298708,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "z0nQpTRDUodX",
    "outputId": "a125b419-1127-4440-f937-aea74a57f877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Retrieved Chunks:\n",
      "\n",
      "Chunk 1 (score=0.8394):\n",
      "۔ 27 ﻣﺋﯽ ، \n",
      "ﭘﺎﺋﯽ ﺗﮭﯾں ﺟو درﯾﺎؤں ﮐو ﻣﺳدود اور ڈﯾم ﮐررﮨﯽ ﺗﮭﯾں ، اور ﯾہ اﻧدازه ﻟﮕﺎﯾﺎ ﮔﯾﺎ ﺗﮭﺎ ﮐہ ان ﻣﯾں ﺳﮯ 28 اب ﺑﮭﯽ ﻣﻘﺎﻣﯽ ﻟوﮔوں \n",
      "ﮐﮯ ﻟﺋﮯ ﻣﻣﮑﻧہ ﺧطره ﮨﯾں۔ اس ﮐﮯ ﻧﺗﯾﺟﮯ ﻣﯾں ﺳﯾﻼب ﮐﯽ وﺟہ ﺳﮯ ﻣﮑﻣل دﯾﮩﺎت ﮐو ﺧﺎﻟﯽ ﮐرﻧﺎ ﭘڑا۔ \n",
      "2008\n",
      "رﯾﺎﺳﺗﯽ ﮐوﻧﺳل ﻧﮯ زﻟزﻟﮯ ﮐﮯ ﻣﺗﺎﺛرﯾن ﮐﮯ ﻟﺋﮯ 19 ﻣﺋﯽ \n",
      "اﻋﻼن ﮐﯾﺎ\n",
      "--------------------------------------------------\n",
      "Chunk 2 (score=0.8253):\n",
      "۔ ﭼﯾن ﮐﺎ ﻗوﻣﯽ ﺟﮭﻧڈا \n",
      "ﺳﭨﯾٹ ﮐوﻧﺳل ﻧﮯ 19 ﻣﺋﯽ \n",
      "اور ﮨﺎﻧﮓ ﮐﺎﻧﮓ اور ﻣﮑﺎؤ ﮐﮯ ﺧﺻوﺻﯽ اﻧﺗظﺎﻣﯽ ﻋﻼﻗوں ﮐﮯ ﻋﻼﻗﺎﺋﯽ ﺟﮭﻧڈے آدھﮯ ﻣﺳت ﭘر ﻟﮩراﺋﮯ ﮔﺋﮯ۔ ﯾہ ﭘﮩﻠﯽ ﺑﺎر ﺗﮭﺎ \n",
      "ﮐہ ﮐﺳﯽ رﯾﺎﺳﺗﯽ رﮨﻧﻣﺎ ﮐﯽ ﻣوت ﮐﮯ ﻋﻼوه ﮐﺳﯽ اور ﭼﯾز ﮐﮯ ﻟﺋﮯ ﻗوﻣﯽ ﺳوگ ﮐﺎ اﻋﻼن ﮐﯾﺎ ﮔﯾﺎ ﺗﮭﺎ ، اور ﺑﮩت ﺳﮯ ﻟوﮔوں ﻧﮯ \n",
      "2008\n",
      "اﺳﮯ ﻣﺎؤ زے ڈوﻧﮓ ﮐﯽ ﻣوت ﮐﮯ ﺑﻌد ﺳوگ ﮐﺎ ﺳب ﺳﮯ ﺑڑا ﻣظﺎﮨره ﻗرار دﯾﺎ ﮨﮯ\n",
      "--------------------------------------------------\n",
      "Chunk 3 (score=0.8207):\n",
      "۔ ﺑﮩت ﺳﮯ ﻟوﮔوں ﻧﮯ ﻣوﺑﺎﺋل ﻓون ﭘر ﭨﯾﮑﺳٹ ﻣﯾﺳﺟﻧﮓ ﮐﮯ ذرﯾﻌﮯ ﭼﯾن ﯾوﻧﯾﮑوم اور ﭼﺎﺋﻧﺎ \n",
      "ﻣوﺑﺎﺋل ﮐﮯ ذرﯾﻌہ ﻗﺎﺋم ﮐرده اﮐﺎؤﻧﭨس ﻣﯾں ﻋطﯾہ ﮐﯾﺎ۔ 16 ﻣﺋﯽ ﺗﮏ ، ﭼﯾﻧﯽ ﺣﮑوﻣت ﻧﮯ زﻟزﻟﮯ ﮐﯽ اﻣداد ﮐﮯ ﻟﺋﮯ اب ﺗﮏ \n",
      "ﻣﺟﻣوﻋﯽ طور ﭘر 772 ﻣﻠﯾن ڈاﻟر ﻣﺧﺗص ﮐﯾﮯ ﺗﮭﮯ ، ﺟو 14 ﻣﺋﯽ ﺳﮯ 159 ﻣﻠﯾن ڈاﻟر ﺳﮯ ﺗﯾزی ﺳﮯ ﺑڑھ ﮔﯾﺎ ﺗﮭﺎ۔ \n",
      "2008\n",
      "رﯾﺎﺳﺗﯽ ﮐوﻧﺳل ﻧﮯ زﻟزﻟﮯ ﮐﮯ ﻣﺗﺎﺛرﯾن ﮐﮯ ﻟﺋﮯ 19 ﻣﺋﯽ \n",
      "اﻋﻼن ﮐﯾﺎ\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/2720202295.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    }
   ],
   "source": [
    "doc_collection_name = \"docus_chunks\" \n",
    "query = \"19 مئی 2008 کو کون سی تقریب کا اعلان کیا گیا؟\"\n",
    "\n",
    "chunks, chunk_scores = query_similarity_docus_chunks(client, doc_collection_name, query, top_k=3)\n",
    "\n",
    "print(\"Top 3 Retrieved Chunks:\\n\")\n",
    "for i, (chunk, score) in enumerate(zip(chunks, chunk_scores), 1):\n",
    "    print(f\"Chunk {i} (score={score:.4f}):\\n{chunk}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-2A7gRABlER"
   },
   "source": [
    "# **Finding Semantic Similarity of Query with Q&As**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731935298708,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "gP3sPs1rGRHo"
   },
   "outputs": [],
   "source": [
    "# Function to query Qdrant for top-k similar embeddings\n",
    "def query_similar_embeddings(client, collection_name, query, top_k=5):\n",
    "    model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "    query_embedding = model.encode([query])[0].tolist()  # Generate query embedding\n",
    "\n",
    "    # Perform the search to get the most similar vectors\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k  # Number of nearest neighbors to retrieve\n",
    "    )\n",
    "\n",
    "    # Retrieve answers from the search results\n",
    "    answers = [result.payload['answer'] for result in search_results]\n",
    "    scores = [result.score for result in search_results]\n",
    "\n",
    "    return answers , scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 642,
     "status": "ok",
     "timestamp": 1731935299345,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "-8wysrbDFOoh",
    "outputId": "2cf75cc7-1866-42fc-f6eb-403f16d5521d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/1452528721.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Answers\n",
      "1: اسٹیٹ کونسل نے زلزلے کے متاثرین کے لیے تین روزہ قومی سوگ کا اعلان کیا۔\n",
      "2: 27 مئی، 2008 تک زلزلے کے ملبے کی وجہ سے 34 جھیلیں تشکیل پائی تھیں جو دریاؤں کو مسدود اور ڈیم کر رہی تھیں۔\n",
      "3: سرکاری عہدیداروں نے ہزاروں گرے ہوئے اسکولوں کے کھنڈرات کا معائنہ کرنا شروع کیا۔\n",
      "4: 7 اپریل 2008 کو تین کارکنوں نے تبتی جھنڈے لے کر گولڈن گیٹ برج کی معطلی کیبلوں پر چڑھائی کی۔\n",
      "5: 8 اپریل کو متعدد احتجاجوں کی منصوبہ بندی کی گئی تھی۔\n"
     ]
    }
   ],
   "source": [
    "# Query similar questions to a given query\n",
    "query = \"19 مئی 2008 کو کون سی تقریب کا اعلان کیا گیا؟\"\n",
    "\n",
    "retrieved_answers , scores = query_similar_embeddings(client, collection_name, query)\n",
    "\n",
    "\n",
    "# Display retrieved answers\n",
    "print(\"Retrieved Answers\")\n",
    "for i, answer in enumerate(retrieved_answers, 1):\n",
    "    print(f\"{i}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1731935299345,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "iDt4VMd1TMbv",
    "outputId": "e04fc608-6f0f-41c8-e8c1-e46a7abea129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Scores\n",
      "1: 0.934968578690951\n",
      "2: 0.9125134389921301\n",
      "3: 0.9104740274318642\n",
      "4: 0.900152235411595\n",
      "5: 0.8996248410774526\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved Scores\")\n",
    "for i, answer in enumerate(scores, 1):\n",
    "    print(f\"{i}: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzBucbLZSDsP"
   },
   "source": [
    "### **Generating response from Retrived_Answers using Alif 8b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(9463) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.9/site-packages (1.79.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.9/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.9/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.9/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/mac/Documents/LMA-RAG Code/LMA-RAG Thesis Code/OpenAI FAQ/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(9465) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./venv/lib/python3.9/site-packages (1.79.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.9/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.9/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.9/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/mac/Documents/LMA-RAG Code/LMA-RAG Thesis Code/OpenAI FAQ/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "openai_client = OpenAI(api_key= \"sk-\")  # or use environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_openai(context, query, model_name=\"gpt-4o\"):\n",
    "    prompt = f\"\"\"\n",
    "You are given a **Urdu question** and a relevant **Urdu context**. Read the context carefully and generate a **correct, concise answer in Urdu**, based **only** on the information in the context.\n",
    "\n",
    "Instructions:\n",
    "- Use only the information provided in the context.\n",
    "- Do **not** add any external or inferred information.\n",
    "- Answer should be brief, accurate, and in **Urdu only**.\n",
    "\n",
    "### Example:\n",
    "\n",
    "**Question:**\n",
    "19 مئی 2008 کو کون سی تقریب کا اعلان کیا گیا؟\n",
    "\n",
    "**Context:**\n",
    "چین کا قومی جھنڈا سٹیٹ کونسل نے 19 مئی کو آدھے مست پر لہرایا۔ یہ پہلی بار تھا کہ کسی ریاستی رہنما کی موت کے علاوہ کسی اور چیز کے لیے قومی سوگ کا اعلان کیا گیا تھا۔ ریاستی کونسل نے زلزلے کے متاثرین کے لیے 19 مئی کو اعلان کیا۔\n",
    "\n",
    "**Answer:**\n",
    "19 مئی 2008 کو زلزلے کے متاثرین کے لیے تین روزہ قومی سوگ کا اعلان کیا گیا۔\n",
    "\n",
    "---\n",
    "\n",
    "Now answer the following:\n",
    "\n",
    "**Question:**\n",
    "{query}\n",
    "\n",
    "**Context:**\n",
    "{context}\n",
    "\n",
    "**Answer (in Urdu):**\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error from OpenAI API:\", e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اسٹیٹ کونسل نے زلزلے کے متاثرین کے لیے تین روزہ قومی سوگ کا اعلان کیا۔',\n",
       " '27 مئی، 2008 تک زلزلے کے ملبے کی وجہ سے 34 جھیلیں تشکیل پائی تھیں جو دریاؤں کو مسدود اور ڈیم کر رہی تھیں۔',\n",
       " 'سرکاری عہدیداروں نے ہزاروں گرے ہوئے اسکولوں کے کھنڈرات کا معائنہ کرنا شروع کیا۔',\n",
       " '7 اپریل 2008 کو تین کارکنوں نے تبتی جھنڈے لے کر گولڈن گیٹ برج کی معطلی کیبلوں پر چڑھائی کی۔',\n",
       " '8 اپریل کو متعدد احتجاجوں کی منصوبہ بندی کی گئی تھی۔']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: 19 مئی 2008 کو زلزلے کے متاثرین کے لیے تین روزہ قومی سوگ کا اعلان کیا گیا۔\n"
     ]
    }
   ],
   "source": [
    "query = \"19 مئی 2008 کو کون سی تقریب کا اعلان کیا گیا؟\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Generate an answer using the local Ollama server\n",
    "    final_answer = generate_using_openai(retrieved_answers, query)\n",
    "    print(\"Generated Answer:\", final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSayKTfb3zQm"
   },
   "source": [
    "# **RAG Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1731939581312,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "jWkp7Zvy3coo"
   },
   "outputs": [],
   "source": [
    "def traditional_rag_pipeline(client, trad_collection_name, query):\n",
    "    \"\"\"\n",
    "    Executes the traditional RAG approach.\n",
    "    Returns results, retrieval time, and generation time.\n",
    "    \"\"\"\n",
    "    start_retrieval = time.time()\n",
    "    retrieved_context , similarity_scores = query_similarity_docus_chunks(client, trad_collection_name, query, top_k=3)  # Retrieve relevant sentences\n",
    "    end_retrieval = time.time()\n",
    "    retrieval_time = end_retrieval - start_retrieval\n",
    "\n",
    "    start_answer = time.time()\n",
    "    generated_answer = generate_using_openai(retrieved_context, query)  # Generate answer\n",
    "    end_answer = time.time()\n",
    "    generation_time = end_answer - start_answer\n",
    "\n",
    "    total_time = retrieval_time + generation_time\n",
    "\n",
    "    result = {\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"retrieval_time\": retrieval_time,\n",
    "        \"answer\": generated_answer,  # Make sure this key is correctly returned\n",
    "        \"generation_time\": generation_time,\n",
    "        \"total_time\": total_time,\n",
    "        \"similarity score\": similarity_scores\n",
    "    }\n",
    "\n",
    "    # Debug: Print the result to inspect the return value\n",
    "    print(\"Traditional RAG Result Generated\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731935301325,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "kIV8-R7F3xDr"
   },
   "outputs": [],
   "source": [
    "def mod_rag_pipeline(query, client, mod_collection_name, top_k=3):\n",
    "    \"\"\"\n",
    "    Executes the second RAG approach using query embeddings.\n",
    "    Returns results, retrieval time, and other metadata.\n",
    "    \"\"\"\n",
    "    start_retrieval = time.time()\n",
    "    answers, scores = query_similar_embeddings(client, mod_collection_name, query, top_k)  # Now returns two lists\n",
    "    end_retrieval = time.time()\n",
    "    retrieval_time = end_retrieval - start_retrieval\n",
    "\n",
    "    if answers:\n",
    "        # Combine answers with their corresponding similarity scores for context\n",
    "        retrieved_documents = [f\"Answer: {answers[i]}\\nScore: {scores[i]:.2f}\" for i in range(len(answers))]\n",
    "\n",
    "        # Generate answer based on retrieved answers\n",
    "        start_answer = time.time()\n",
    "        generated_answer = generate_using_openai(answers, query)\n",
    "        end_answer = time.time()\n",
    "        answer_time = end_answer - start_answer\n",
    "\n",
    "        total_time = retrieval_time + answer_time\n",
    "\n",
    "        return {\n",
    "            \"retrieved_documents\": retrieved_documents,\n",
    "            \"similarity_scores\": scores,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"generation_time\": answer_time,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"retrieved_documents\": [],\n",
    "            \"similarity_scores\": [],\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"generated_answer\": None,\n",
    "            \"generation_time\": 0,\n",
    "            \"total_time\": retrieval_time\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1731935301325,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "YO3htCRoGwgx"
   },
   "outputs": [],
   "source": [
    "def rag_pipeline(query, client, trad_collection_name, mod_collection_name, similarity_threshold=0.8, top_k=3):\n",
    "    \"\"\"\n",
    "    Main pipeline that first tries query embeddings (approach 2).\n",
    "    Falls back to traditional RAG if similarity scores are below the threshold.\n",
    "    \"\"\"\n",
    "    # Step 1: Try query embeddings (Approach 2)\n",
    "    query_similar_results = mod_rag_pipeline(query, client, mod_collection_name, top_k)\n",
    "\n",
    "    # Check similarity scores\n",
    "    if query_similar_results[\"similarity_scores\"]:\n",
    "        max_similarity = max(query_similar_results[\"similarity_scores\"])\n",
    "\n",
    "        if max_similarity >= similarity_threshold:\n",
    "            # Return results from the query embeddings approach\n",
    "            return {\n",
    "                \"case\": \"Q-A Index\",\n",
    "                \"answer\": query_similar_results[\"generated_answer\"],\n",
    "                \"retrieved_context\": \"\\n\".join(query_similar_results[\"retrieved_documents\"]),\n",
    "                \"retrieval_time\": query_similar_results[\"retrieval_time\"],\n",
    "                \"generated_context\": query_similar_results[\"generated_answer\"],\n",
    "                \"generation_time\": query_similar_results[\"generation_time\"],\n",
    "                \"total_time\": query_similar_results[\"total_time\"],\n",
    "            }\n",
    "\n",
    "    # Step 2: Fallback to traditional RAG if similarity is low or no results\n",
    "    traditional_results = traditional_rag_pipeline(client, trad_collection_name, query)\n",
    "\n",
    "    return {\n",
    "        \"case\": \"Traditional RAG\",\n",
    "        \"answer\": traditional_results[\"answer\"],\n",
    "        \"retrieved_context\": \"\\n\".join(traditional_results[\"retrieved_context\"]),\n",
    "        \"retrieval_time\": traditional_results[\"retrieval_time\"],\n",
    "        \"generation_time\": traditional_results[\"generation_time\"],\n",
    "        \"total_time\": traditional_results[\"total_time\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3505,
     "status": "ok",
     "timestamp": 1731935468319,
     "user": {
      "displayName": "saad ahmad",
      "userId": "17990399590159369797"
     },
     "user_tz": -300
    },
    "id": "fClO9Bp4VM6N",
    "outputId": "4ca37ea5-3308-440c-eefa-509322a87968"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/1452528721.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: Q-A Index\n",
      "\n",
      "Answer: والدین نے شکایت کی کہ انہیں ابھی تک کوئی رپورٹ موصول نہیں ہوئی۔\n",
      "\n",
      "Retrieved Context: Answer: والدین نے شکایت کی کہ انہیں ابھی تک کوئی رپورٹ موصول نہیں ہوئی۔\n",
      "Score: 0.99\n",
      "Answer: احتجاج کو محدود کرنے کے لئے عہدیداروں نے والدین کو ایک دستاویز پر دستخط کرنے پر مجبور کیا جس میں انہیں احتجاج کرنے سے منع کیا گیا۔\n",
      "Score: 0.87\n",
      "Answer: عہدیداروں نے والدین کو ایک دستاویز پر دستخط کرنے پر مجبور کیا جس میں انہیں احتجاج کرنے سے منع کیا گیا۔\n",
      "Score: 0.87\n",
      "\n",
      "Retrieval Time: 79.28s\n",
      "\n",
      "Generation Time: 2.44s\n",
      "\n",
      "Total Time: 81.72s\n"
     ]
    }
   ],
   "source": [
    "# Execute the RAG pipeline\n",
    "results = rag_pipeline(query=\"والدین نے 17 جولائی 2008 تک کس چیز کی شکایت کی؟\", client=client, trad_collection_name=\"docus_chunks\", mod_collection_name=\"faq_embeddings\", similarity_threshold=0.8, top_k=3)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Case: {results['case']}\\n\")\n",
    "print(f\"Answer: {results['answer']}\\n\")\n",
    "print(f\"Retrieved Context: {results['retrieved_context']}\\n\")\n",
    "print(f\"Retrieval Time: {results['retrieval_time']:.2f}s\\n\")\n",
    "print(f\"Generation Time: {results['generation_time']:.2f}s\\n\")\n",
    "print(f\"Total Time: {results['total_time']:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Ground Truth CSVs and merging results into 1 Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def convert_xlsx_to_csv(xlsx_path, csv_path):\n",
    "#     \"\"\"\n",
    "#     Convert an Excel .xlsx file to a .csv file.\n",
    "\n",
    "#     Args:\n",
    "#         xlsx_path (str): Path to the input .xlsx file.\n",
    "#         csv_path (str): Path where the output .csv will be saved.\n",
    "#     \"\"\"\n",
    "#     df = pd.read_excel(xlsx_path)\n",
    "#     df.to_csv(csv_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_xlsx_to_csv(\"Urdu_qa_groundtruths.xlsx\", \"Urdu_qa_groundtruths.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"Urdu_qa_groundtruths.csv\"\n",
    "merged_df = pd.read_csv(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answer_columns(filtered_df):\n",
    "    # Select only 'question' and 'answers' columns from the merged DataFrame\n",
    "    if 'question' in merged_df.columns and 'answer' in merged_df.columns:\n",
    "        return merged_df[['question', 'answer']]\n",
    "    else:\n",
    "        raise ValueError(\"The columns 'question' and/or 'answer' do not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=get_question_answer_columns(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 اپریل کو اولمپک مشعل کس شہر میں تھی؟</td>\n",
       "      <td>استنبول</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چینی حکومت نے بائیکاٹ کی صورتحال کو کم کرنے کے...</td>\n",
       "      <td>سینسرشپ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008 کے اولمپکس کے لئے پہلے مشعل بردار کا نام ...</td>\n",
       "      <td>الیکسینڈروس نکولائڈس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>کون سی حکومت مشعل کے راستے کی وضاحت کرنے کے لئ...</td>\n",
       "      <td>تائیوان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>قازقستان میں روٹ کے لئے کلومیٹر میں فاصلہ کیا ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>زلزلے کی فوکل گہرائی کتنی تھی؟</td>\n",
       "      <td>19 کلومیٹر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>سیچوان زلزلہ کس سال ہوا تھا؟</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>14 مئی تک کتنی رقم عطیہ کی گئی تھی؟</td>\n",
       "      <td>10.7 بلین یوآن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>زلزلہ دن کے کس وقت ہوا؟</td>\n",
       "      <td>02:28:01 چین اسٹینڈرڈ ٹائم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>تائیوان سے چارٹرڈ فلائٹ کہاں اتری؟</td>\n",
       "      <td>چنگڈو</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0               3 اپریل کو اولمپک مشعل کس شہر میں تھی؟   \n",
       "1    چینی حکومت نے بائیکاٹ کی صورتحال کو کم کرنے کے...   \n",
       "2    2008 کے اولمپکس کے لئے پہلے مشعل بردار کا نام ...   \n",
       "3    کون سی حکومت مشعل کے راستے کی وضاحت کرنے کے لئ...   \n",
       "4    قازقستان میں روٹ کے لئے کلومیٹر میں فاصلہ کیا ...   \n",
       "..                                                 ...   \n",
       "894                     زلزلے کی فوکل گہرائی کتنی تھی؟   \n",
       "895                       سیچوان زلزلہ کس سال ہوا تھا؟   \n",
       "896                14 مئی تک کتنی رقم عطیہ کی گئی تھی؟   \n",
       "897                            زلزلہ دن کے کس وقت ہوا؟   \n",
       "898                 تائیوان سے چارٹرڈ فلائٹ کہاں اتری؟   \n",
       "\n",
       "                         answer  \n",
       "0                       استنبول  \n",
       "1                       سینسرشپ  \n",
       "2          الیکسینڈروس نکولائڈس  \n",
       "3                       تائیوان  \n",
       "4                            20  \n",
       "..                          ...  \n",
       "894                  19 کلومیٹر  \n",
       "895                        2008  \n",
       "896              10.7 بلین یوآن  \n",
       "897  02:28:01 چین اسٹینڈرڈ ٹائم  \n",
       "898                       چنگڈو  \n",
       "\n",
       "[899 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              question  \\\n",
      "0               3 اپریل کو اولمپک مشعل کس شہر میں تھی؟   \n",
      "1    چینی حکومت نے بائیکاٹ کی صورتحال کو کم کرنے کے...   \n",
      "2    2008 کے اولمپکس کے لئے پہلے مشعل بردار کا نام ...   \n",
      "3    کون سی حکومت مشعل کے راستے کی وضاحت کرنے کے لئ...   \n",
      "4    قازقستان میں روٹ کے لئے کلومیٹر میں فاصلہ کیا ...   \n",
      "..                                                 ...   \n",
      "894                     زلزلے کی فوکل گہرائی کتنی تھی؟   \n",
      "895                       سیچوان زلزلہ کس سال ہوا تھا؟   \n",
      "896                14 مئی تک کتنی رقم عطیہ کی گئی تھی؟   \n",
      "897                            زلزلہ دن کے کس وقت ہوا؟   \n",
      "898                 تائیوان سے چارٹرڈ فلائٹ کہاں اتری؟   \n",
      "\n",
      "                         answer  \n",
      "0                       استنبول  \n",
      "1                       سینسرشپ  \n",
      "2          الیکسینڈروس نکولائڈس  \n",
      "3                       تائیوان  \n",
      "4                            20  \n",
      "..                          ...  \n",
      "894                  19 کلومیٹر  \n",
      "895                        2008  \n",
      "896              10.7 بلین یوآن  \n",
      "897  02:28:01 چین اسٹینڈرڈ ٹائم  \n",
      "898                       چنگڈو  \n",
      "\n",
      "[899 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NaN in either 'question' or 'answers' column\n",
    "new_df = new_df.dropna(subset=['question', 'answer']).reset_index(drop=True)\n",
    "\n",
    "# View the cleaned DataFrame\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating results for all Ground Truth Q&As**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
    "# else:\n",
    "#     print(\"CUDA not available, running on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)  # Shows which CUDA version PyTorch was built with (None if CPU-only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(\"PyTorch CUDA available?\", torch.cuda.is_available())\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Answers Generate** ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/1452528721.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n",
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/2720202295.py:24: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 1 queries. Saved partial results to Final_Answers_Generated_OpenAI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sc/f3wmm1ys5zq4wz5yyxpmsxy40000gn/T/ipykernel_7862/1452528721.py:8: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional RAG Result Generated\n",
      "Processed 2 queries. Saved partial results to Final_Answers_Generated_OpenAI.csv\n",
      "Traditional RAG Result Generated\n",
      "Processed 3 queries. Saved partial results to Final_Answers_Generated_OpenAI.csv\n",
      "Traditional RAG Result Generated\n",
      "Processed 4 queries. Saved partial results to Final_Answers_Generated_OpenAI.csv\n",
      "Traditional RAG Result Generated\n",
      "Processed 5 queries. Saved partial results to Final_Answers_Generated_OpenAI.csv\n",
      "Traditional RAG Result Generated\n",
      "Processed 6 queries. Saved partial results to Final_Answers_Generated_OpenAI.csv\n"
     ]
    }
   ],
   "source": [
    "new_df['modified_rag_refined_answer'] = \"\"\n",
    "new_df['modified_rag_case'] = \"\"\n",
    "new_df['modified_rag_retrieval_time'] = \"\"\n",
    "new_df['modified_rag_generation_time'] = \"\"\n",
    "new_df['modified_rag_total_time'] = \"\"\n",
    "\n",
    "new_df['traditional_rag_retrieved_context'] = \"\"\n",
    "new_df['traditional_rag_refined_answer'] = \"\"\n",
    "new_df['traditional_rag_retrieval_time'] = \"\"\n",
    "new_df['traditional_rag_generation_time'] = \"\"\n",
    "new_df['traditional_rag_total_time'] = \"\"\n",
    "\n",
    "output_file = 'Final_Answers_Generated_OpenAI.csv'\n",
    "\n",
    "it = 0\n",
    "for index, row in new_df.iterrows():\n",
    "    query = row['question']  # Extract the question from the CSV\n",
    "\n",
    "    # Pass the query through the RAG pipeline\n",
    "    rag_result = rag_pipeline(\n",
    "        query, \n",
    "        client=client, \n",
    "        trad_collection_name=\"docus_chunks\", \n",
    "        mod_collection_name=\"faq_embeddings\", \n",
    "        similarity_threshold=0.8, \n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    # Extract RAG results\n",
    "    retrival_time = rag_result[\"retrieval_time\"]\n",
    "    generation_time = rag_result[\"generation_time\"]\n",
    "    total_time = rag_result[\"total_time\"]\n",
    "    retrieved_context = rag_result[\"retrieved_context\"]\n",
    "    refined_answer = rag_result[\"answer\"]\n",
    "    case = rag_result[\"case\"]\n",
    "\n",
    "    # Update the DataFrame directly for the current row\n",
    "    new_df.at[index, 'modified_rag_retrieved_context'] = retrieved_context\n",
    "    new_df.at[index, 'modified_rag_refined_answer'] = refined_answer\n",
    "    new_df.at[index, 'modified_rag_case'] = case\n",
    "    new_df.at[index, 'modified_rag_retrieval_time'] = f\"{retrival_time:.2f}\"\n",
    "    new_df.at[index, 'modified_rag_generation_time'] = f\"{generation_time:.2f}\"\n",
    "    new_df.at[index, 'modified_rag_total_time'] = f\"{total_time:.2f}\"\n",
    "\n",
    "    # Traditional RAG approach\n",
    "    traditional_results = traditional_rag_pipeline(client, \"docus_chunks\", query)\n",
    "    trad_rag_retrieved_context_str = \"\\n\\n\".join(traditional_results[\"retrieved_context\"])\n",
    "    trad_refined_answer = traditional_results[\"answer\"]\n",
    "    trad_retrieval_time = traditional_results[\"retrieval_time\"]\n",
    "    trad_generation_time = traditional_results[\"generation_time\"]\n",
    "    trad_total_time = traditional_results[\"total_time\"]\n",
    "\n",
    "    # Update DataFrame columns for traditional RAG\n",
    "    new_df.at[index, 'traditional_rag_retrieved_context'] = trad_rag_retrieved_context_str\n",
    "    new_df.at[index, 'traditional_rag_refined_answer'] = trad_refined_answer\n",
    "    new_df.at[index, 'traditional_rag_retrieval_time'] = f\"{trad_retrieval_time:.2f}\"\n",
    "    new_df.at[index, 'traditional_rag_generation_time'] = f\"{trad_generation_time:.2f}\"\n",
    "    new_df.at[index, 'traditional_rag_total_time'] = f\"{trad_total_time:.2f}\"\n",
    "\n",
    "\n",
    "    new_df.to_csv(output_file, index=False,encoding=\"utf-8-sig\")\n",
    "\n",
    "    it += 1\n",
    "    print(f\"Processed {it} queries. Saved partial results to {output_file}\")\n",
    "\n",
    "print(\"All queries processed. Final results saved to:\", output_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1PoJ2eXvfpybgZMYLuXRB2VodilOLSIAC",
     "timestamp": 1731917087961
    },
    {
     "file_id": "1lt7bRC58P4uVjLE4loNBBFxofRbcZhP0",
     "timestamp": 1731911135884
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
